import os
import time
import csv
from concurrent.futures import ThreadPoolExecutor, as_completed
from tabulate import tabulate

class FolderSizeAnalyzer:
    def __init__(self, root_dir, max_level=4, size_threshold_gb=1, export_csv=False, max_workers=8):
        self.root_dir = root_dir
        self.max_level = max_level
        self.size_threshold = size_threshold_gb * (1024 ** 3)  # GB to bytes
        self.export_csv = export_csv
        self.max_workers = max_workers
        self.results = []
        self.executor = ThreadPoolExecutor(max_workers=max_workers)  # shared pool

    def get_folder_size(self, path):
        """Faster recursive size calculation using scandir with parallel subdir scanning."""
        total = 0
        subdirs = []

        try:
            for entry in os.scandir(path):
                try:
                    if entry.is_file(follow_symlinks=False):
                        total += entry.stat(follow_symlinks=False).st_size
                    elif entry.is_dir(follow_symlinks=False):
                        subdirs.append(entry.path)
                except Exception:
                    continue
        except Exception:
            return 0

        # Process subdirectories in parallel
        if subdirs:
            futures = [self.executor.submit(self.get_folder_size, subdir) for subdir in subdirs]
            for f in as_completed(futures):
                try:
                    total += f.result()
                except Exception:
                    continue

        return total

    def process_directory(self, path, level):
        """Process a single directory and return its result if it exceeds threshold."""
        size = self.get_folder_size(path)
        if size >= self.size_threshold:
            return {
                "Level": level,
                "Directory": path,
                "Size (GB)": round(size / (1024 ** 3), 2)
            }
        return None

    def analyze_directory(self, path, current_level):
        """Collect subdirectories up to max_level."""
        if current_level > self.max_level:
            return []
        try:
            return [
                (os.path.join(path, d), current_level)
                for d in os.listdir(path)
                if os.path.isdir(os.path.join(path, d))
            ]
        except Exception:
            return []

    def run(self):
        print(f"Scanning {self.root_dir} up to level {self.max_level}...\n")
        work_queue = [(self.root_dir, 1)]
        futures = []

        while work_queue:
            path, level = work_queue.pop()
            futures.append(self.executor.submit(self.process_directory, path, level))

            # Queue subdirectories for further processing
            for subdir, lvl in self.analyze_directory(path, level + 1):
                work_queue.append((subdir, lvl))

        for future in as_completed(futures):
            result = future.result()
            if result:
                self.results.append(result)

        # Sort results by size
        self.results.sort(key=lambda x: x["Size (GB)"], reverse=True)

        # Display results
        print(tabulate(self.results, headers="keys", tablefmt="grid"))

        if self.export_csv:
            self.export_to_csv(self.results)

        self.executor.shutdown(wait=True)

    def export_to_csv(self, results):
        file_path = "Directory_Report.csv"
        with open(file_path, mode='w', newline='') as file:
            writer = csv.DictWriter(file, fieldnames=["Level", "Directory", "Size (GB)"])
            writer.writeheader()
            writer.writerows(results)
        print(f"\nReport saved to {file_path}")


if __name__ == "__main__":
    start = time.time()
    analyzer = FolderSizeAnalyzer(
        root_dir=r"C:\Users\h59341",
        size_threshold_gb=1,
        max_level=4,
        export_csv=True,
        max_workers=8  # tune based on CPU & disk speed
    )
    analyzer.run()
    end = time.time()
    print(f"Processing time: {(end - start) / 60:.2f} minutes")
